{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CartPole Environment\n",
    "\n",
    "### Details\n",
    "* Name: CartPole-v0  \n",
    "* Category: Classic Control\n",
    "* [Leaderboard Page](https://github.com/openai/gym/wiki/Leaderboard#cartpole-v0)\n",
    "* Old links:\n",
    "  * [Environment Page](https://gym.openai.com/envs/CartPole-v0)  \n",
    "  * [Algorithms Page](https://gym.openai.com/algorithms?groups=classic_control)\n",
    "\n",
    "### Description\n",
    "A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum starts upright, and the goal is to prevent it from falling over by increasing and reducing the cart's velocity.\n",
    "\n",
    "### Source\n",
    "This environment corresponds to the version of the cart-pole problem described by Barto, Sutton, and Anderson \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation\n",
    "Type: Box(4)\n",
    "\n",
    "Num | Observation | Min | Max\n",
    "---|---|---|---\n",
    "0 | Cart Position | -2.4 | 2.4\n",
    "1 | Cart Velocity | -Inf | Inf\n",
    "2 | Pole Angle | ~ -41.8&deg; | ~ 41.8&deg;\n",
    "3 | Pole Velocity At Tip | -Inf | Inf\n",
    "\n",
    "### Starting State\n",
    "All observations are assigned a uniform random value between ±0.05\n",
    "\n",
    "### Code\n",
    "\n",
    "> #### Box Space는 Continuous Space\n",
    "> - Q-Network를 구성한다면, state가 Continuous한 상태로 문제를 풀 수 있으나, Q-table을 이용할 경우 discrete하게 나눌 필요\n",
    "> - 이 경우, reward 산정에 필요한 state는 **Pole Angle**과 **Pole Velocity At Tip**\n",
    "> - Cart관련 state는 구간을 나눌 필요 없음\n",
    "> - **Pole Angle**은 6개의 구간으로 나눔\n",
    "> - **Pole Velocity At Tip**은 3개의 구간으로 나눔\n",
    "> - 나눈 구간은 Q-table작성시 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_BUCKETS = (1, 1, 6, 3)\n",
    "STATE_BOUNDS = list(zip(env.observation_space.low, env.observation_space.high))\n",
    "STATE_BOUNDS[1] = [-0.5, 0.5]\n",
    "STATE_BOUNDS[3] = [-math.radians(50), math.radians(50)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions\n",
    "Type: Discrete(2)\n",
    "\n",
    "Num | Action\n",
    "--- | ---\n",
    "0 | Push cart to the left\n",
    "1 | Push cart to the right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ACTIONS = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward\n",
    "Reward is 1 for every step taken, including the termination step\n",
    "\n",
    "### Code\n",
    "\n",
    "> #### Q-table은 state 경우의 수 * discrete action 경우의 수 만큼 필요\n",
    "> - (1, 1, 6, 3, 2)차원의 array를 Q-table로 활용\n",
    "> - Q-table 0으로 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = np.zeros(NUM_BUCKETS + (NUM_ACTIONS,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Episode Termination\n",
    "1. Pole Angle is more than ±12°\n",
    "2. Cart Position is more than ±2.4 (center of the cart reaches the edge of the display)\n",
    "3. Episode length is greater than 200\n",
    "\n",
    "### Solved Requirements\n",
    "Considered solved when the average reward is greater than or equal to 195.0 over 100 consecutive trials.\n",
    "\n",
    "### Code\n",
    "\n",
    "> #### 학습 변수 세팅\n",
    "> - 최소 epsilon = 0.01\n",
    "> - 최소 학습률 = 0.1\n",
    "> - 진행 episode = 1000 episode\n",
    "> - episode length = 250 timestep\n",
    "> - 성공조건 = timestep 200 달성\n",
    "> - 종료조건 = 120회 연속으로 성공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_EXPLORE_RATE = 0.01\n",
    "MIN_LEARNING_RATE = 0.1\n",
    "\n",
    "NUM_EPISODES = 1000\n",
    "MAX_T = 250\n",
    "SOLVED_T = 199\n",
    "STREAK_TO_END = 120\n",
    "\n",
    "DEBUG_MODE = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decay되는 변수(explore rate, learning rate) 함수 정의\n",
    "\n",
    "- min, max clip\n",
    "- $y(t) = 1 - \\log{{t+1}\\over{25}}$\n",
    "\n",
    "<img src=\"./img/decay_graph.png\" alt=\"graph\" width=\"450\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_explore_rate(t):\n",
    "    if t >= 24:\n",
    "        return max(MIN_EXPLORE_RATE, min(1, 1.0 - math.log10((t+1)/25)))\n",
    "    else:\n",
    "        return 1.0\n",
    "    \n",
    "def get_learning_rate(t):\n",
    "    if t >= 24:\n",
    "         return max(MIN_LEARNING_RATE, min(0.5, 1.0 - math.log10((t+1)/25)))\n",
    "    else:\n",
    "         return 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State Discretization\n",
    "\n",
    "- state를 bucket index로 mapping\n",
    "- bound 기준으로 벗어나는 값들은 양 끝단 index로\n",
    "- bound내를 bucket수만큼의 구간으로 나누어 mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_to_bucket(state):\n",
    "    bucket_indice = []\n",
    "    for i in range(len(state)):\n",
    "        if state[i] <= STATE_BOUNDS[i][0]:\n",
    "            bucket_index = 0\n",
    "        elif state[i] >= STATE_BOUNDS[i][1]:\n",
    "            bucket_index = NUM_BUCKETS[i] - 1\n",
    "        else:\n",
    "            bound_width = STATE_BOUNDS[i][1] - STATE_BOUNDS[i][0]\n",
    "            offset = (NUM_BUCKETS[i]-1)*STATE_BOUNDS[i][0]/bound_width\n",
    "            scaling = (NUM_BUCKETS[i]-1)/bound_width\n",
    "            bucket_index = int(round(scaling*state[i] - offset))\n",
    "        bucket_indice.append(bucket_index)\n",
    "    return tuple(bucket_indice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy\n",
    "\n",
    "- $\\epsilon$-greedy\n",
    "- $\\epsilon=$ explore_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state, explore_rate):\n",
    "    if random.random() < explore_rate:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        action = np.argmax(q_table[state])\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm\n",
    "\n",
    "- Q-learning (off-policy)\n",
    "    - greedy action을 대상으로 업데이트함\n",
    "\n",
    "    ```python\n",
    "best_q = np.amax(q_table[state])  \n",
    "q_table[state_0 + (action,)] += \n",
    "    learning_rate * (reward + discount_factor * (best_q) - q_table[state_0 + (action,)])  \n",
    "```\n",
    "\n",
    "- SARSA일 경우? (on-policy)\n",
    "    - policy에 따라 선택한 action을 대상으로 업데이트함\n",
    "    ```python\n",
    "next_action = select_action(state, explore_rate) \n",
    "policy_q = q_table[state][next_action]\n",
    "q_table[state_0 + (action,)] += \n",
    "    learning_rate * (reward + discount_factor * (policy_q) - q_table[state_0 + (action,)])  \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate():\n",
    "    \n",
    "    learning_rate = get_learning_rate(0) \n",
    "    # learning rate 초기값 설정\n",
    "    \n",
    "    explore_rate = get_explore_rate(0) \n",
    "    # explore rate 초기값 설정\n",
    "    \n",
    "    discount_factor = 0.99  \n",
    "    # discount factor 설정\n",
    "\n",
    "    num_streaks = 0 \n",
    "    # 연속성공 카운터\n",
    "\n",
    "    for episode in range(NUM_EPISODES): \n",
    "    # NUM_EPISODES만큼 episode 진행\n",
    "\n",
    "        obv = env.reset() \n",
    "        # state 초기화\n",
    "        \n",
    "        state_0 = state_to_bucket(obv) \n",
    "        # state discretization\n",
    "\n",
    "        for t in range(MAX_T): \n",
    "        # episode length = MAX_T\n",
    "        \n",
    "            env.render() \n",
    "            # environment 렌더링 (그림)\n",
    "\n",
    "            action = select_action(state_0, explore_rate) \n",
    "            # action 선택\n",
    "            \n",
    "            obv, reward, done, _ = env.step(action) \n",
    "            # 선택한 action environment에서 시행, state/reward 획득\n",
    "            \n",
    "            state = state_to_bucket(obv) \n",
    "            # state discretization\n",
    "\n",
    "            best_q = np.amax(q_table[state]) \n",
    "            # 해당 state (전이된 state)에서 Q최대값 선택\n",
    "            \n",
    "            q_table[state_0 + (action,)] += learning_rate*(reward + discount_factor*(best_q) - q_table[state_0 + (action,)])\n",
    "            # Q최대값을 이용하여 Q-table 업데이트 (이전 state)\n",
    "\n",
    "            state_0 = state \n",
    "            # state 업데이트\n",
    "\n",
    "            if (DEBUG_MODE):\n",
    "                print(\"\\nEpisode = %d\" % episode)\n",
    "                print(\"t = %d\" % t)\n",
    "                print(\"Action: %d\" % action)\n",
    "                print(\"State: %s\" % str(state))\n",
    "                print(\"Reward: %f\" % reward)\n",
    "                print(\"Best Q: %f\" % best_q)\n",
    "                print(\"Explore rate: %f\" % explore_rate)\n",
    "                print(\"Learning rate: %f\" % learning_rate)\n",
    "                print(\"Streaks: %d\" % num_streaks)\n",
    "                print(\"\")\n",
    "                # logging\n",
    "\n",
    "            if done:\n",
    "                print(\"%f 번의 steps 이후에 Episode %d이 끝났음\" % (t, episode))\n",
    "                if (t >= SOLVED_T):\n",
    "                    num_streaks += 1\n",
    "                    # 성공조건 만족시 연속성공 카운팅\n",
    "                else:\n",
    "                    num_streaks = 0\n",
    "                    # 성공조건 불만족시 카운터 초기화\n",
    "                break\n",
    "\n",
    "\n",
    "        if num_streaks > STREAK_TO_END:\n",
    "            break\n",
    "            # 종료조건 만족시 종료\n",
    "\n",
    "        explore_rate = get_explore_rate(episode)\n",
    "        learning_rate = get_learning_rate(episode)\n",
    "        # 진행 episode에 맞는 explore rate 및 learning rate로 업데이트\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
