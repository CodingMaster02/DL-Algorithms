{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cart Pole 강화학습 연습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/cartpole_env.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 필요한 모듈 import\n",
    "* gym (가상환경을 제공) \n",
    "* numpy (텐서 연산)\n",
    "* matplotlib (결과 그래프 출력)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 가상환경 설정 \n",
    "\n",
    "* gym.make를 이용하여 등록된 'CartPole-v0' 환경을 env로 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  hyper parameter 정의\n",
    "* learning rate init: 초기 학습률 (시간 경과에 따른 학습률을 조정하기 위해)\n",
    "* discount: 감쇄계수\n",
    "* num_episodes: 전체 반복 학습 횟수\n",
    "* max_trial: 실패하기 전까지 반복횟수\n",
    "* num_success: 1차 학습 완료까지 기준\n",
    "* streak_num_success: 1차성공 (199번 반복시까지 실패가 없을 경우)이 연속된 횟수\n",
    "* status_check: 진행상황 확인 (IF TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate_init = 1.0\n",
    "dis = 0.95\n",
    "\n",
    "num_episodes = 1000\n",
    "max_trial = 200\n",
    "num_success = 199\n",
    "streak_num_success = 100\n",
    "\n",
    "status_check = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Episode 종료\n",
    "\n",
    "1. 폴의 기울어진 각도가 ±15°보다 크거나 작을 때\n",
    "2. 카트의 위치가 ±2.4 를 벗어날 때\n",
    "3. 에피소드의 시도 횟수가 200회 이상일 때\n",
    "\n",
    "\n",
    "## 학습 완료 기준\n",
    "\n",
    "* 진행 episode = 1000 episode\n",
    "* episode length = 200 timestep\n",
    "* 성공조건 = timestep 199 달성\n",
    "* 종료조건 = 100회 연속으로 성공"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CartPole-v0에서 정의한 State Boundary 및 Action Set\n",
    "\n",
    "<img src=\"./img/cartpole_env_boundary.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CartPole-v0 환경이 정의한 상태 경계값\n",
    "STATE_BOUNDS = list(zip(env.observation_space.low, env.observation_space.high))\n",
    "print(\"states boundary: \", env.observation_space.low, env.observation_space.high )\n",
    "print(\"STATE_BOUNDS [Min, Max]\", STATE_BOUNDS)\n",
    "\n",
    "# 상태 경계값을 조정 \n",
    "STATE_BOUNDS[1] = [-0.5, 0.5]\n",
    "STATE_BOUNDS[3] = [-1, 1]\n",
    "\n",
    "# Action set 정의\n",
    "num_actions = env.action_space.n\n",
    "print(\"number of actions: \", num_actions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Table 작성을 위한 State Discretization\n",
    "\n",
    "- Q-Table의 State: (S1,S2,S3,S4) := (카트 위치, 카트 수평속도, 폴의 기울기 각도, 폴의 각속도)\n",
    "- 가상환경 env에서 만들어주는 연속적인 상태를 Q-Table 불연속 테이를로 매핑\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-Table에서의 상태 테이블\n",
    "Q_State = (1, 1, 6, 3)\n",
    "\n",
    "# Q-Table 정의\n",
    "Q_Table = np.zeros(Q_State + (num_actions,))\n",
    "\n",
    "# 연속적인 가상환경 ENV 상태를 불연속인 Q_STATE롷 변환\n",
    "def state_to_table(state):\n",
    "    q_state = []\n",
    "    for i in range(len(state)):\n",
    "        if state[i] <= STATE_BOUNDS[i][0]:\n",
    "            state_index = 0\n",
    "            \n",
    "        elif state[i] >= STATE_BOUNDS[i][1]:\n",
    "            state_index = Q_State[i] - 1\n",
    "            \n",
    "        else:\n",
    "            bound_width = STATE_BOUNDS[i][1] - STATE_BOUNDS[i][0]\n",
    "            offset = (Q_State[i]-1)*STATE_BOUNDS[i][0]/bound_width\n",
    "            scaling = (Q_State[i]-1)/bound_width\n",
    "            state_index = int(round(scaling*state[i] - offset))\n",
    "        q_state.append(state_index)\n",
    "    return tuple(q_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cart Pole 가상환경 관찰\n",
    "* env.reset(): 가상환경 초기화 \n",
    "* env.rener(): 가상환경 표현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "for i in range(100):\n",
    "    random_action = env.action_space.sample()\n",
    "    new_state, reward, done, info = env.step(random_action)\n",
    "    env.render()\n",
    "    if done:\n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning 학습\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 연속성공 카운터\n",
    "num_streaks = 0 \n",
    "\n",
    "# num_episodes만큼 반복학습\n",
    "for i in range(num_episodes): \n",
    "\n",
    "    # 환경 초기화    \n",
    "    obs = env.reset() \n",
    "\n",
    "    # state discretization\n",
    "    state = state_to_table(obs)\n",
    "\n",
    "    # e-greedy 계산 (expore_rate)\n",
    "    e = 1. / ( i//20 + 1 )\n",
    "    \n",
    "    \n",
    "    # learning rate decay factor 계산 \n",
    "    df = 1. / ( i//20 + 1 )\n",
    "    learning_rate = df*learning_rate_init\n",
    "    \n",
    "    # max_trial = 200\n",
    "    for t in range(max_trial): \n",
    "\n",
    "        # 현재 상태 표현\n",
    "        env.render() \n",
    "        \n",
    "        # e-greedy에 의한 행동 설정\n",
    "        if np.random.rand(1) < e:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(Q_Table[state])\n",
    "            \n",
    "        # 행동을 하면 환경이 새로운 상태를 보여줌 (보상값은 새로운 환경을 기준으로 사람이 설정)    \n",
    "        obs, reward, done, _ = env.step(action) \n",
    "\n",
    "        # 연속적인 환경 변수에서 테이블로 표현된 불연속 Q-Table로 변경\n",
    "        new_state = state_to_table(obs)\n",
    "\n",
    "        # maxQ 값 선정\n",
    "        maxQ = np.amax(Q_Table[new_state])\n",
    "            \n",
    "        # Q-Learning 공식을 이용한 Q-Table 업데이트    \n",
    "        Q_Table[state + (action,)] += learning_rate*(reward + dis*maxQ - Q_Table[state + (action,)])\n",
    "            \n",
    "        state = new_state \n",
    "\n",
    "        # 진행상태 확인\n",
    "        if (status_check):\n",
    "            print(\"\\nEpisode = %d\" % i)\n",
    "            print(\"t = %d\" % t)\n",
    "            print(\"explore rate:\", e, \"learning rate: \", learning_rate)\n",
    "            print(\"Action: %d\" % action)\n",
    "            print(\"State: %s\" % str(state))\n",
    "            print(\"Observation: %s\" % str(obs))\n",
    "            print(\"Reward: %f\" % reward)\n",
    "            print(\"Best Q: %f\" % maxQ)\n",
    "            print(\"Streaks: %d\" % num_streaks)\n",
    "            print(\"\")\n",
    "            \n",
    "        if done:\n",
    "            # 성공조건 만족시 연속성공 카운팅 (199번 연속 안 쓰러뜨릴 경우)\n",
    "            if (t >= num_success):\n",
    "                num_streaks += 1\n",
    "            \n",
    "            # 성공조건 불만족시 카운터 초기화\n",
    "            else:\n",
    "                num_streaks = 0\n",
    "                \n",
    "            break\n",
    "\n",
    "    # 학습완료조건 만족시 종료\n",
    "    if num_streaks > streak_num_success:\n",
    "        break\n",
    "         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
