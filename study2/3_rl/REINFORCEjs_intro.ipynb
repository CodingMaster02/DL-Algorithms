{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REINFORCEjs\n",
    "\n",
    "### by Andrej Karpathy\n",
    "\n",
    "(https://cs.stanford.edu/people/karpathy/)  \n",
    "(https://github.com/karpathy)\n",
    "\n",
    "https://cs.stanford.edu/people/karpathy/reinforcejs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gridworld DP\n",
    "\n",
    "$\\gamma = 0.9$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/griddp.png\" alt=\"griddp\" width=\"800\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy evaluation\n",
    "\n",
    "#### (x=3, y=4) 의 다음 state value V는?\n",
    "\n",
    "- 상: 0 + 0 = 0\n",
    "- 하: 0 + 0 = 0\n",
    "- 좌: 0 + 0 = 0\n",
    "- 우: 0 + (-1) * 0.9 = -0.9\n",
    "- (상+하+좌+우) / 4 = -0.225\n",
    "\n",
    "#### (x=4, y=4) 의 다음 state value V는?\n",
    "\n",
    "- 상: (-1) + (-1) * 0.9 = -1.9\n",
    "- 하: (-1) + 0 = -1\n",
    "- 좌: (-1) + 0 = -1\n",
    "- 우: (-1) + (-1) * 0.9 = -1.9\n",
    "- (상+하+좌+우) / 4 = -1.45\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Dynamic Programming은 Model을 알아야 한다\" 의 의미\n",
    "\n",
    "- agent가 직접 state 위에서 action을 취하여 episode를 만들어내며 학습하지 않음\n",
    "- model자체의 정보 (transition probability, reward정보 등)을 통하여 value를 업데이트하는 것이 목적\n",
    "- 경험을 통하여 업데이트하지 않고, 미리 알고 있는 정보를 통하여 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gridworld TD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가장 먼저 봐야할 점 : agent가 직접 움직여 경험을 통해 학습한다는 점"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-learning (off-policy) vs SARSA (on-policy)\n",
    "\n",
    "- Q-learning\n",
    "    - $Q(a,s)\\leftarrow Q(a,s)+\\alpha(r_s+\\gamma\\max_{a'}Q(a',s')-Q(a,s))$\n",
    "    - optimal하지 않은 value 무시\n",
    "\n",
    "- SARSA\n",
    "    - $Q(a,s)\\leftarrow Q(a,s)+\\alpha(r_s+\\gamma Q(a',s')-Q(a,s))$\n",
    "    - optimal하지 않은 value도 update\n",
    "    \n",
    "#### 효과\n",
    "\n",
    "- Q-learning은 optimal path만 찾으려 함\n",
    "- SARSA는 근처에 hazardous state가 있을 경우 근처의 모든 state가 영향을 받음\n",
    "    - dangerous path로부터 최대한 떨어져 우회하려고 함\n",
    "    \n",
    "- 이러한 효과는 cliff-walking problem에 대하여 두 에이전트를 적용해 보면 쉽게 볼 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REINFORCEjs에 기본값으로 설계되어 있는 Gridworld에는 절벽이 존재  \n",
    "\n",
    "<img src=\"./img/griddp1.png\" alt=\"griddp1\" width=\"400\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-learning은 위험을 회피하려는 습성이 없기 때문에 쉽게 optimal path를 찾으나,  \n",
    "SARSA는 위험을 회피하려는 습성 탓에 이런 문제에 대하여 학습이 어려움"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "반면, reward가 state space에 대하여 점진적으로 변화할 경우  \n",
    "SARSA가 Q-learning보다 optimal path에 빠르게 접근할 확률이 높음  \n",
    "(optimal path 근처의 높은 reward값들에 대한 update가 이루어지기 때문)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "어떤 알고리즘을 적용해야 할 것인가에 대해서는  \n",
    "**환경에 대한 충분한 이해**  \n",
    "를 바탕으로 해당 환경에 대하여 어떠한 알고리즘이 가장 적절한지에 대하여 고찰해봐야 할 것"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
